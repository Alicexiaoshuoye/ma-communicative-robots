{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30baadd-ba94-4172-82b8-e252bb329d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43971fb5-189e-4a2d-8998-3c2ca969e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_data = []\n",
    "with open(\"semeval-2018-task4/dat/friends.train.scene_delim.conll\") as fp:\n",
    "    for line in list(fp):\n",
    "        #print(repr(line))\n",
    "        new = line.split()\n",
    "        #print(new)\n",
    "        comma_data.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "61b5013d-6bc4-4381-bd27-18fb6457b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_list = ['he', 'she', 'it', 'they', 'him', 'her', 'them', 'his', 'her', 'hers', 'its', 'their', 'theirs']\n",
    "ignored_references = ['i', 'me', 'you', 'my', 'mine', 'myself', 'your', 'yourself', 'yours']\n",
    "relations = ['brother', 'sister', 'sibling', 'mother', 'father', 'child', 'friend', 'daddy', 'mom', 'dad', 'boyfriend', 'girlfriend','grandmother','grandfather', 'son', 'daughter', 'aunt', 'uncle', 'date', 'wife', 'husband', 'partner']\n",
    "#nickname or other name for the person > e.g. boy for birthday boy. These seem to be mostly for talking to someone directly, not about another person. So maybe ignore these?\n",
    "nickname = ['sweetie', 'guy', 'woman', 'boy', 'man', 'girl', 'honey', 'pal', 'one', 'kid','baby','princess', 'queen','bitch']\n",
    "names = ['Joey','Rachel','Monica','Ross','Phoebe','Chandler']\n",
    "last_names = ['Tribbiani', 'Green', 'Geller', 'Buffay', 'Bing', 'Willick', 'Bunch', 'Goodacre', 'Farber']\n",
    "reference_types = ['F', 'R', 'P', 'S', 'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d3cc24b7-0794-47f3-b011-93293c6d72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_per_sentence(data):\n",
    "    sentences = []\n",
    "    one_sentence = []\n",
    "    for lst in data:\n",
    "        if not lst:\n",
    "            one_sentence.append(lst)\n",
    "            sentences.append(one_sentence)\n",
    "            one_sentence = []\n",
    "        else:\n",
    "            one_sentence.append(lst)\n",
    "    return sentences\n",
    "\n",
    "def find_identifiers_per_token(data, identifier, person, speakers=['Monica_Geller', 'Joey_Tribbiani', 'Phoebe_Buffay', 'Rachel_Green', 'Chandler_Bing', 'Ross_Geller']):\n",
    "    '''\n",
    "    Assigns reference types to each token for the given identifier and person (should represent the same person).\n",
    "    '''\n",
    "    token_patterns = []\n",
    "    for lst in data:\n",
    "        if not lst:\n",
    "            token_patterns.append('new')\n",
    "        elif lst[-2] == \"part\" or lst[-2] == 'document':\n",
    "            token_patterns.append('new_scene')\n",
    "        elif lst[-1] != '-' and lst[-1] != '(DATE)':   #add more when I find more > person, organization, etc.? or this is [-2] instead of [-1]\n",
    "            if lst[-2] != 'part' and lst[-2] != 'document':\n",
    "                if lst[-1] == identifier[:-1]:\n",
    "                    if lst[3] in names or lst[3] == person:\n",
    "                        token_patterns.append('A')\n",
    "                elif lst[-1] == identifier:\n",
    "                    if lst[-3] in speakers:\n",
    "                        if lst[3].lower() not in ignored_references:\n",
    "                            if lst[3] in pronoun_list:\n",
    "                                token_patterns.append('P')\n",
    "                            elif lst[3].lower() in relations:\n",
    "                                token_patterns.append('R')\n",
    "                            elif lst[3] in names or lst[3] == person:\n",
    "                                token_patterns.append('F')\n",
    "                            elif lst[3] in last_names:\n",
    "                                token_patterns.append('S')\n",
    "                            else:\n",
    "                                token_patterns.append('.')\n",
    "                        else:\n",
    "                            token_patterns.append('.')\n",
    "                    else:\n",
    "                        token_patterns.append('.')\n",
    "                else:\n",
    "                    token_patterns.append('.')\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            token_patterns.append('.')\n",
    "    return token_patterns\n",
    "\n",
    "def into_sentences(data):\n",
    "    '''\n",
    "    Outputs the sentences as lists of tokens.\n",
    "    '''\n",
    "    sentences = []\n",
    "    one_sentence = []\n",
    "    for lst in data:\n",
    "        for lst2 in lst:\n",
    "            if lst2 and lst2[0] != '#begin' and lst2[0] != '#end':\n",
    "                one_sentence.append(lst2[3])\n",
    "        sentences.append(one_sentence)\n",
    "        one_sentence = []\n",
    "    return sentences\n",
    "\n",
    "def list_into_string_lists(lst, split):\n",
    "    '''\n",
    "    To turn one list with each token being one element into a list of elements that are strings. To make the patterns.\n",
    "    '''\n",
    "    string = ''.join([str(elem) for elem in lst])\n",
    "    splitted = string.split(split)\n",
    "    return splitted\n",
    "\n",
    "def reference_per_utterance(lst):\n",
    "    \"\"\"\n",
    "    If there exists a reference type in an utterance, keep only those. Utterances without reference type become a period.\n",
    "    \"\"\"\n",
    "    ref_per_utterance = []\n",
    "    for pattern in lst:\n",
    "        pattern_contains_ref = any(reference_type in pattern for reference_type in reference_types)\n",
    "        #print(pattern_contains_ref)\n",
    "        if not pattern_contains_ref:\n",
    "            ref_per_utterance.append('.')\n",
    "        else:\n",
    "            ref_per_utterance.append(pattern.replace('.',''))\n",
    "    \n",
    "    return ref_per_utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "335b5dfb-f5dc-41d1-967e-78d7f3e053e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = find_identifiers_per_token(comma_data, identifier='(248)', person=\"Monica\")\n",
    "scenes = list_into_string_lists(test_list, 'new_scene')\n",
    "\n",
    "list_of_scene_utterances = []\n",
    "for scene in scenes:\n",
    "    test = list_into_string_lists(scene, 'new')\n",
    "    list_of_scene_utterances.append(test)\n",
    "    \n",
    "#print(list_of_scene_utterances)\n",
    "    \n",
    "utterance_patterns_per_scene = []\n",
    "for lst in list_of_scene_utterances:\n",
    "    test2 = reference_per_utterance(lst)\n",
    "    utterance_patterns_per_scene.append(test2)\n",
    "    \n",
    "#print(utterance_patterns_per_scene[1:])\n",
    "\n",
    "string_utterance_patterns_per_scene = []\n",
    "for scene in utterance_patterns_per_scene:\n",
    "    test3 = ''.join(scene)\n",
    "    string_utterance_patterns_per_scene.append(test3)\n",
    "    \n",
    "#print(string_utterance_patterns_per_scene[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "44ff5581-9561-4357-9bc2-6b29cc725ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#human annotated and generated patterns that were manually found\n",
    "human_annotated = ['FPR........F', 'FF........', '.APRP..FPPF....', '..FPR....A..', '...RF......', '..F....F..', '....FR.....', '.....RF....', '......RF...', '.......FR..', '.......AFR.', 'RF..F......', '.A.....FF..', '..RF.......', '...ARF.....', '...RF......', '....A.FP...', '.....FRF...', '.......APP..', 'AR', 'FR', 'AP', 'FP.F', 'AF.....FF', 'AR', 'RFF...FR', 'RF', 'APR', 'APP', 'FFR', 'AAR', 'RF', 'FPR', 'RFPFR', 'F.F', 'AR.....F', 'RFPFR', 'AR', 'A.F', 'AFFR.F', 'A........F', 'FFR', 'RFFR..RF', 'AP.F', 'F.F', 'RF', 'RAF..F', 'RF', 'FR...FR', 'AR..FR', 'A.....F', 'FR', 'RFPA', 'RF....F', 'RF', 'APP', 'AR', 'A..F', 'FR.FP.F..A', 'R.....F', 'RA...F', 'AP', 'A.......FFR', 'AR..F', 'A.....F', 'APPP', 'FR', 'A..F', 'APRP...F', 'FPR..F', 'FPR....FPR', 'RFF', 'AR']\n",
    "generated = ['FFR........AFR', '.F..AFRFFR....', '..AFR....AFR..', '....F.A...', 'AA', 'AFR', 'FFR', '.A......FF', '.AFR......A.', 'FFR...A', 'FF.....A', 'AFR', 'FFRFFR', 'AFRPFR', 'FFR', 'AFR', 'PFR.FF', 'FFR......FFR', 'AAFRA...AFR', 'A..F', 'AFR', 'AA..A...AF', 'FFF', 'AFR', 'AFR', 'PFR....PFR', 'AFF...A', 'AFR.FFR', 'FA.A', 'FAFR.FFR', 'AFR........AFR', 'FFR', 'A..A..FF', 'FFR.F', 'FFR', 'FFR', 'F.PF', 'FFA..A', 'FFR..AFR', 'FF...FR', 'F..FFR', 'FFR.....F', 'PFR', 'PFR', 'AFR', 'AFR', 'PFR', 'AFRAFAFR', 'AFR', 'F..A', 'A.......AFR', 'AAA', 'FFR', 'FFR.A..FFR..A', 'A.PFR...FFR', 'FFR.....AFR', 'A.....F', 'FFRPFR', 'FF', 'AA', 'PFR', 'PFR.......FFR', 'FFR', 'AAFR', 'FA..FFR', 'F.....FFR', 'FFR', 'FFRAFR', 'A.....A', 'FFR..A..A', 'A..PFR..FFR', 'AFRF...F', 'F....AFR', 'FFR', 'FA', 'FFR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "d5a45d30-0791-46b2-97f3-06301461d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will find patterns in the text (so for .....P...P..FF....). Find the mean values? input should be utterance_patterns_per_scene\n",
    "def find_patterns(data, first_compare, second_compare):\n",
    "    '''\n",
    "    Calculates the distance between the first reference type and the second reference type. \n",
    "    '''\n",
    "    distances = []\n",
    "    for scene in data:\n",
    "        distance = 0\n",
    "        for token in scene:\n",
    "            if token == first_compare and distance == 0:\n",
    "                distances.append('start')\n",
    "            elif not distances:\n",
    "                continue\n",
    "            elif token == \".\" and distances[-1] == 'start':\n",
    "                distance+=1\n",
    "            elif token == second_compare and distances[-1] == 'start':\n",
    "                distance+=1\n",
    "                distances.append(distance)\n",
    "                distance = 0\n",
    "                if first_compare == second_compare:\n",
    "                    distances.append('start')\n",
    "\n",
    "    distances = [x for x in distances if x != 'start']\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "55c9a0bf-d229-40a6-aa7e-0c7ad2341182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70, 2]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of the distance between R and P, as well as how often the combation occurs\n",
    "find_patterns(string_utterance_patterns_per_scene, \"R\", \"P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "7827f7f8-bb46-46f0-a5c9-db04af7222aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_empty_patterns(pattern_data):\n",
    "    '''\n",
    "    Find patterns that don't have a zero value as the frequency.\n",
    "    '''\n",
    "    first = second = [\"P\", \"F\", \"A\", \"S\", \"R\"]\n",
    "    not_empty = []\n",
    "    for i in first:\n",
    "        for j in second:\n",
    "            lst = find_patterns(pattern_data, i, j)\n",
    "            if not lst:\n",
    "                pass\n",
    "            else:\n",
    "                not_empty.append([i,j])\n",
    "    return not_empty\n",
    "\n",
    "#non_empty_lists = find_non_empty_patterns(string_utterance_patterns_per_scene)\n",
    "#non_empty_lists = find_non_empty_patterns(human_annotated)\n",
    "non_empty_lists = find_non_empty_patterns(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "93395033-92cd-44f9-9609-ca52d61a9865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'P']\n",
      "3\n",
      "4\n",
      "['P', 'F']\n",
      "1\n",
      "13\n",
      "['P', 'A']\n",
      "1.5\n",
      "10\n",
      "['P', 'R']\n",
      "1\n",
      "13\n",
      "['F', 'P']\n",
      "1.6153846153846154\n",
      "13\n",
      "['F', 'F']\n",
      "4.4324324324324325\n",
      "37\n",
      "['F', 'A']\n",
      "2.36734693877551\n",
      "49\n",
      "['F', 'R']\n",
      "1.3376623376623376\n",
      "77\n",
      "['A', 'P']\n",
      "1.4\n",
      "10\n",
      "['A', 'F']\n",
      "1.7959183673469388\n",
      "49\n",
      "['A', 'A']\n",
      "4.2592592592592595\n",
      "27\n",
      "['A', 'R']\n",
      "1.4285714285714286\n",
      "42\n",
      "['R', 'P']\n",
      "1.6153846153846154\n",
      "13\n",
      "['R', 'F']\n",
      "2.276315789473684\n",
      "76\n",
      "['R', 'A']\n",
      "2.380952380952381\n",
      "42\n",
      "['R', 'R']\n",
      "4.56\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def get_mean(non_empty_list, pattern_list):\n",
    "    for i in non_empty_list:\n",
    "        print(i)\n",
    "        print(statistics.mean(find_patterns(pattern_list, i[0], i[1])))\n",
    "        print(len(find_patterns(pattern_list, i[0], i[1])))\n",
    "              \n",
    "#get_mean(non_empty_lists, string_utterance_patterns_per_scene)\n",
    "#get_mean(non_empty_lists, human_annotated)\n",
    "get_mean(non_empty_lists, generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "e393719e-5ac7-40ec-8b57-44dbb417eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(find_patterns(utterance_patterns_per_scene, \"P\", \"S\"))\n",
    "#print(comma_data[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "68145447-634c-4b19-bc3b-d446748607ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vague_references(data, identifiers=['(248)', '(355)', '(183)', '(292)', '(59)', '(306)', '(271)'], persons=['Monica', 'Joey','Rachel','Phoebe','Chandler','Ross','Judy'], speakers=['Monica_Geller', 'Joey_Tribbiani', 'Phoebe_Buffay', 'Rachel_Green', 'Chandler_Bing', 'Ross_Geller']):\n",
    "    '''\n",
    "    Find vagueness counts. If an utterance contains a reference to a person without it being their name or one of the \n",
    "    ignored identifiers, it will add that.\n",
    "    '''\n",
    "    #wat moet: als de identifier een van de friends is en niet de naam is, kijk of zijn/haar naam wordt eerder in die zin wordt genoemd.\n",
    "    sentence = []\n",
    "    number_of_sentences=0\n",
    "    list_number_of_sentences=[]\n",
    "    vagueness=0\n",
    "    vagueness_list = []\n",
    "    for lst in data:\n",
    "        if not lst:\n",
    "            sentence=[]\n",
    "            number_of_sentences+=1\n",
    "        elif lst[-2] == \"part\" or lst[-2] == 'document':\n",
    "            vagueness_list.append(vagueness)\n",
    "            vagueness=0\n",
    "            list_number_of_sentences.append(number_of_sentences)\n",
    "            number_of_sentences=0\n",
    "        elif lst[-1] != '-' and lst[-1] != '(DATE)':   #add more when I find more > person, organization, etc.? or this is [-2] instead of [-1]\n",
    "            if lst[-2] != 'part' and lst[-2] != 'document':\n",
    "                for identifier in identifiers:\n",
    "                    if lst[-1] == identifier[:-1]:\n",
    "                        if lst[3] in names or lst[3] in persons:\n",
    "                            pass\n",
    "                    elif lst[-1] == identifier:\n",
    "                        if lst[-3] in speakers:\n",
    "                            if lst[3].lower() not in ignored_references:\n",
    "                                if lst[3] in names or lst[3] in persons:\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    if lst[3] not in sentence:\n",
    "                                        vagueness += 1\n",
    "                                        sentence.append(lst[3])\n",
    "                                        #print(sentence)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return vagueness_list, list_number_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "d536cedf-c6f6-4977-a38a-fcb8df62c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of a vague utterance and how many sentences there are in total\n",
    "vagueness,sentences=find_vague_references(comma_data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "580fad88-13e1-461b-be2f-5b86867f4a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.021052631578947368, 0.08411214953271028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08571428571428572, 0.0, 0.0, 0.0, 0.060240963855421686, 0.018518518518518517, 0.0, 0.0, 0.0, 0.0, 0.07894736842105263, 0.037037037037037035, 0.0, 0.028169014084507043, 0.0, 0.0, 0.12121212121212122, 0.0, 0.21052631578947367, 0.03225806451612903, 0.0, 0.0, 0.06451612903225806, 0.02702702702702703, 0.01818181818181818, 0.043478260869565216, 0.0, 0.043478260869565216, 0.047619047619047616, 0.0, 0.0, 0.061224489795918366, 0.07692307692307693, 0.0, 0.0, 0.07692307692307693, 0.025974025974025976, 0.0, 0.09090909090909091, 0.09375, 0.12727272727272726, 0.08333333333333333, 0.21739130434782608, 0.04, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.06976744186046512, 0.029411764705882353, 0.0, 0.0, 0.04, 0.038461538461538464, 0.0, 0.029850746268656716, 0.0, 0.0625, 0.13793103448275862, 0.0, 0.0, 0.0, 0.0, 0.015873015873015872, 0.018867924528301886, 0.0, 0.0, 0.0, 0.0, 0.01639344262295082, 0.03125, 0.08928571428571429, 0.06060606060606061, 0.015625, 0.0, 0.0, 0.022222222222222223, 0.0, 0.07142857142857142, 0.12, 0.2692307692307692, 0.05263157894736842, 0.04878048780487805, 0.0, 0.028169014084507043, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14814814814814814, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.027777777777777776, 0.1206896551724138, 0.0625, 0.0, 0.019417475728155338, 0.0, 0.033707865168539325, 0.12962962962962962, 0.0, 0.0, 0.021739130434782608, 0.0, 0.023809523809523808, 0.05454545454545454, 0.0, 0.0, 0.0, 0.11764705882352941, 0.07692307692307693, 0.15789473684210525, 0.0, 0.0, 0.056818181818181816, 0.021739130434782608, 0.061224489795918366, 0.0, 0.0, 0.0759493670886076, 0.03488372093023256, 0.1875, 0.017543859649122806, 0.02631578947368421, 0.0, 0.041666666666666664, 0.0, 0.4666666666666667, 0.06382978723404255, 0.043478260869565216, 0.0, 0.0, 0.0, 0.0, 0.01282051282051282, 0.02127659574468085, 0.0, 0.023809523809523808, 0.0, 0.0, 0.045454545454545456, 0.23076923076923078, 0.05, 0.019230769230769232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.2, 0.06779661016949153, 0.1724137931034483, 0.0, 0.013888888888888888, 0.0, 0.0, 0.0410958904109589, 0.2, 0.0, 0.03571428571428571, 0.05263157894736842, 0.02040816326530612, 0.0, 0.06060606060606061, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.02702702702702703, 0.041666666666666664, 0.0, 0.0, 0.023809523809523808, 0.0, 0.037037037037037035, 0.0, 0.4444444444444444, 0.029411764705882353, 0.05, 0.12121212121212122, 0.034482758620689655, 0.02564102564102564, 0.07142857142857142, 0.0, 0.08571428571428572, 0.0, 0.0, 0.0, 0.043478260869565216, 0.05405405405405406, 0.038461538461538464, 0.0, 0.125, 0.0851063829787234, 0.030303030303030304, 0.01818181818181818, 0.0, 0.3125, 0.03225806451612903, 0.0, 0.04, 0.05, 0.0, 0.04716981132075472, 0.022222222222222223, 0.037037037037037035, 0.012658227848101266, 0.14285714285714285, 0.0, 0.03225806451612903, 0.0, 0.11428571428571428, 0.0, 0.07692307692307693, 0.0, 0.2839506172839506, 0.0, 0.06349206349206349, 0.0, 0.0, 0.12195121951219512, 0.13636363636363635, 0.10869565217391304, 0.03333333333333333, 0.04411764705882353, 0.023255813953488372, 0.17142857142857143, 0.0, 0.10714285714285714, 0.02564102564102564, 0.0, 0.0, 0.02702702702702703, 0.0, 0.0, 0.044444444444444446, 0.0, 0.0, 0.04, 0.04081632653061224, 0.0, 0.025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03225806451612903, 0.0, 0.058823529411764705, 0.0, 0.0, 0.0, 0.03125, 0.18181818181818182, 0.47058823529411764, 0.03571428571428571, 0.15217391304347827, 0.0, 0.125, 0.10909090909090909, 0.13636363636363635, 0.14285714285714285, 0.0, 0.023809523809523808, 0.02702702702702703, 0.0, 0.047619047619047616, 0.13513513513513514, 0.0425531914893617, 0.0, 0.047619047619047616, 0.020833333333333332, 0.0625, 0.0, 0.0, 0.13157894736842105, 0.1282051282051282, 0.07407407407407407, 0.05660377358490566, 0.058823529411764705, 0.058823529411764705, 0.0, 0.05263157894736842, 0.029411764705882353, 0.5, 0.4, 0.0, 0.13333333333333333, 0.07407407407407407, 0.10526315789473684, 0.05, 0.0, 0.07692307692307693, 0.0, 0.041666666666666664, 0.037037037037037035, 0.0, 0.0, 0.0, 0.01098901098901099, 0.0, 0.04, 0.0, 0.07692307692307693, 0.057692307692307696, 0.0, 0.0, 0.0, 0.0, 0.045454545454545456, 0.05263157894736842, 0.0, 0.02857142857142857, 0.08571428571428572, 0.0, 0.0, 0.03571428571428571, 0.06896551724137931, 0.1111111111111111, 0.024390243902439025, 0.0967741935483871, 0.0, 0.0, 0.10869565217391304, 0.03571428571428571, 0.03125, 0.125, 0.05084745762711865, 0.021739130434782608, 0.08695652173913043, 0.05714285714285714, 0.0, 0.041666666666666664, 0.1, 0.0, 0.19230769230769232, 0.0, 0.2727272727272727, 0.0, 0.40540540540540543, 0.18181818181818182, 0.09523809523809523, 0.022222222222222223, 0.0, 0.024390243902439025, 0.14285714285714285, 0.02631578947368421, 0.04]\n"
     ]
    }
   ],
   "source": [
    "vagueness_score = [int(vague) / int(sentence) for vague,sentence in zip(vagueness, sentences)]\n",
    "print(vagueness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "ea833e3f-ecbf-4347-9217-651cfefccc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04673370851759234"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vagueness score\n",
    "sum(vagueness_score)/len(vagueness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "fe7cb66d-d59c-4db4-b572-9e406a63f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_MOA_scores(data, identifiers=['(248)', '(355)', '(183)', '(292)', '(59)', '(306)', '(271)'], persons=['Monica', 'Joey','Rachel','Phoebe','Chandler','Ross','Judy'], speakers=['Monica_Geller', 'Joey_Tribbiani', 'Phoebe_Buffay', 'Rachel_Green', 'Chandler_Bing', 'Ross_Geller']):\n",
    "    '''\n",
    "    Finds the MOA score per utterance. Per scene, I create a list of persons that have been mentioned.\n",
    "    For every ambiguous reference, it looks at how many persons have been mentioned before that.\n",
    "    '''\n",
    "    #wat moet: voor elke pronoun, kijk hoeveel personen daarvoor zijn gereferenced (lastig omdat er geen identifier is voor wie man en vrouw is, dus ik doe alleen de 6 friends\n",
    "    sentence = []\n",
    "    number_of_sentences=0\n",
    "    list_number_of_sentences=[]\n",
    "    vagueness=0\n",
    "    vagueness_list = []\n",
    "    \n",
    "    mentioned = 0\n",
    "    mentioned_list = []\n",
    "    MOA_list = []\n",
    "    MOA_score = 0\n",
    "    count = 0\n",
    "    for lst in data:\n",
    "        if not lst:\n",
    "            sentence=[]\n",
    "            number_of_sentences+=1\n",
    "        elif lst[-2] == \"part\" or lst[-1] == 'document':\n",
    "            MOA_list.append(MOA_score)\n",
    "            mentioned=0\n",
    "            mentioned_list = []\n",
    "            list_number_of_sentences.append(number_of_sentences)\n",
    "            number_of_sentences=0\n",
    "        elif lst[-1] != '-' and lst[-1] != '(DATE)':   #add more when I find more > person, organization, etc.? or this is [-2] instead of [-1]\n",
    "            if lst[-2] != 'part' and lst[-2] != 'document':\n",
    "                if lst[-3] in speakers:\n",
    "                    if lst[-2] == '(PERSON)':\n",
    "                        if lst[-1] in mentioned_list:\n",
    "                            pass\n",
    "                        else:\n",
    "                            mentioned+=1\n",
    "                            mentioned_list.append(lst[-1])\n",
    "                            #print(mentioned_list)\n",
    "                    \n",
    "                    else:\n",
    "                            if lst[3].lower() == 'she' or lst[3].lower() == 'her' or lst[3].lower() == 'he' or lst[3].lower() == 'his':\n",
    "                                MOA_score = mentioned\n",
    "                                count +=1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    #print(mentioned_list)\n",
    "    return MOA_list, list_number_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "45276587-e7f2-469f-9cc6-828a10dcc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOA,sentences=find_MOA_scores(comma_data[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "cf76e005-779e-4933-b7bf-4f5b45e48cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 3, 5, 5, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 4, 4, 4, 4, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 0, 0, 0, 0, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 4, 4, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 4, 4, 0, 0, 3, 3, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 6, 6, 6, 6, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 6, 6, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 2, 2, 4, 4, 0, 0, 2, 2, 2, 2, 2, 2, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 4, 4, 4, 4, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 3, 3, 3, 3, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 5, 5, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 7, 7, 1, 1, 4, 4, 5, 5, 1, 1, 4, 4, 3, 3, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 2, 2, 3, 3, 0, 0, 2, 2, 3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 2, 2, 3, 3, 5, 5, 8, 8, 5, 5, 5, 5, 5, 5, 1, 1, 0, 0, 0, 0, 0, 0, 4, 4, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 4, 4, 4, 4, 2, 2, 3, 3, 0, 0, 0, 0, 0, 0, 6, 6, 0, 0, 1, 1, 0, 0, 1, 1, 2, 2, 4, 4, 4, 4, 3, 3, 0, 0, 1, 1, 1, 1, 0, 0, 3, 3, 1, 1, 6, 6, 2, 2, 2, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 2, 2, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 0, 0, 4, 4, 5, 5, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 3, 3, 5, 5, 2, 2, 4, 4, 1, 1, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 4, 4, 4, 4, 5, 5, 5, 5, 4, 4, 1, 1, 2, 2, 2, 2, 1, 1, 5, 5, 5, 5, 5, 5, 2, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "print(MOA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "77019ca7-0b53-4377-aa36-2cb1c9b0f4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 139, 3: 106, 5: 36, 2: 136, 4: 77, 1: 240, 6: 10, 7: 2, 8: 2})"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(MOA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "209e1212-2299-411e-8089-8e2c17834fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408\n"
     ]
    }
   ],
   "source": [
    "print((3*106)+(5*36)+(2*136)+(4*77)+240+60+14+16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "894bc63c-99bd-4ad7-9812-bd95ac6c4dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 107, 20, 15, 27, 27, 14, 70, 15, 18, 37, 83, 54, 27, 19, 44, 31, 38, 81, 9, 71, 24, 26, 33, 20, 19, 93, 21, 22, 31, 37, 55, 23, 18, 23, 21, 14, 36, 49, 13, 9, 31, 13, 77, 83, 33, 32, 55, 48, 23, 100, 42, 19, 37, 17, 50, 43, 34, 32, 28, 25, 78, 14, 67, 30, 32, 29, 57, 21, 23, 22, 63, 53, 32, 59, 33, 24, 61, 64, 56, 33, 64, 46, 39, 45, 22, 14, 25, 26, 19, 41, 28, 71, 51, 24, 30, 21, 47, 25, 22, 27, 33, 48, 30, 12, 33, 36, 58, 64, 55, 103, 19, 89, 54, 30, 22, 46, 38, 42, 55, 24, 25, 20, 51, 39, 19, 38, 28, 88, 46, 49, 16, 22, 79, 86, 16, 57, 38, 20, 24, 70, 15, 47, 23, 21, 71, 30, 28, 78, 47, 51, 42, 52, 30, 22, 26, 20, 52, 12, 45, 42, 22, 26, 32, 33, 25, 59, 29, 21, 72, 56, 22, 73, 20, 34, 28, 38, 49, 45, 33, 41, 21, 11, 20, 11, 26, 37, 48, 38, 24, 42, 25, 27, 10, 9, 34, 60, 33, 58, 39, 28, 32, 35, 25, 28, 17, 46, 37, 26, 28, 16, 47, 33, 55, 26, 16, 31, 31, 25, 40, 73, 106, 90, 27, 79, 21, 29, 31, 36, 70, 25, 13, 53, 81, 22, 63, 38, 14, 41, 22, 46, 30, 68, 43, 35, 33, 28, 39, 51, 69, 74, 32, 35, 45, 30, 47, 25, 49, 59, 40, 21, 18, 14, 34, 18, 36, 26, 14, 27, 31, 10, 31, 13, 34, 53, 10, 24, 32, 11, 34, 28, 46, 24, 24, 55, 22, 28, 23, 42, 37, 31, 42, 37, 47, 36, 21, 48, 16, 20, 20, 38, 39, 27, 53, 17, 34, 24, 19, 34, 8, 10, 21, 30, 27, 19, 40, 16, 26, 16, 24, 27, 20, 39, 17, 91, 27, 50, 20, 26, 52, 18, 18, 15, 25, 22, 38, 34, 35, 35, 48, 8, 56, 29, 36, 41, 31, 37, 16, 46, 28, 64, 32, 59, 46, 23, 35, 28, 24, 10, 35, 26, 35, 33, 49, 37, 22, 21, 45, 31, 41, 14, 38, 25]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1:-1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "2f108382-2c63-4477-b70b-a4e5bedae9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.21447721179624"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sentences[1:-1:2])/len(sentences[1:-1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "4adf3048-5e7a-4fa9-ac0c-d70821f12393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13556\n"
     ]
    }
   ],
   "source": [
    "print(sum(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7246dc7-9032-46c7-b6c0-d1c92c2a665f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
